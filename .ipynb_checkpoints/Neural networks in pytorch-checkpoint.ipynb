{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2b7db2a",
   "metadata": {},
   "source": [
    "# Neural networks in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e23688f",
   "metadata": {},
   "source": [
    "This notebook explores fitting a one-input, one-output, single-hidden layer neural-network by gradient descent, using automatic differentiation in pytorch.  It is intended to help me understand how neural networks work.  I would to gradually build it up to working with MNIST and transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c44936",
   "metadata": {},
   "source": [
    "With a wide hidden layer you can clearly see the function approach a gaussian process.  It's pretty cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e6569",
   "metadata": {},
   "source": [
    "The problem is essentially to find a network of an appropriate arhcitecture and complexity and efficiently find parameters that are close to globally optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9feb54",
   "metadata": {},
   "source": [
    "The model can get stuck in a local optimum, where the gradient is close to zero, but the loss is still high.  The things that effect it are:\n",
    "- hidden layer width\n",
    "- parameter initialisation\n",
    "- learning rate\n",
    "\n",
    "What is the best way to do optimise these features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea4ddc9",
   "metadata": {},
   "source": [
    "### Hidden layer width (and network depth?)\n",
    "\n",
    "Layer width trades off the flexibility and computational cost of the function.  It should be wide enough to fit the target function but not much wider as that is computationally inefficient.  Excess width also allows over-fitting, but it is better to control this with early-stopping and regularisation using validation sets.\n",
    "\n",
    "This should largely apply to network depth as well, except that the ratio of width to depth is also supposed to matter according to that Principles of DL theory book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be65e4ea",
   "metadata": {},
   "source": [
    "### Parameter intialisation\n",
    "\n",
    "This is the prior on the initial function.  The obvious strategy is to standardise the data and set a reasonable prior on that basis.  It might be good just to write the math for a simple case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d0cd0b",
   "metadata": {},
   "source": [
    "### Learning rate\n",
    "\n",
    "Learning rate should be lower when number of parameters and batch size is higher.\n",
    "\n",
    "There are learning rate finding algorithms, the idea is fairly obvious.  Try a few and choose the ones that decrease the loss most quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72779fc8",
   "metadata": {},
   "source": [
    "**It might be good to go and check some resources and start taking notes on what people are actually doing before I go much further here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe4ecb6",
   "metadata": {},
   "source": [
    "## Neural network with single hidden layer fitted by gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056632b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfc8b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise parameters.  Number of hidden neurons, and weights and biases for hidden layer and output\n",
    "def ip(n_h):\n",
    "    n_h = n_h\n",
    "    W1 = torch.randn((1, n_h), requires_grad=True)\n",
    "    b1 = torch.randn((1, n_h), requires_grad=True)\n",
    "    W2 = torch.randn((n_h, 1), requires_grad=True)\n",
    "    b2 = torch.randn((1, 1), requires_grad=True)\n",
    "    return n_h, W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d13031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data, hidden laer neuron outputs, and network predictions\n",
    "def plot_net(ax, x, y, n_h, h, preds):\n",
    "    ax.scatter(x, y, alpha=0.2)\n",
    "    for i in range(n_h):\n",
    "        ax.plot(x, h[:, i].detach(), alpha=0.1, color='grey')\n",
    "    ax.plot(x, preds.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033f52f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gradient descent, plotting network outputs before and after, and training loss\n",
    "def net_gd(n_h, W1, b1, W2, b2, lr, hist):\n",
    "#     Initial predictions\n",
    "    h = torch.max(torch.zeros((100, n_h)), x @ W1 + b1)\n",
    "    preds = h @ W2 + b2\n",
    "    \n",
    "#     Plot initial predictions\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    plot_net(ax1, x, y, n_h, h, preds)\n",
    "\n",
    "#     Run gradient descent\n",
    "    for i in range(100):\n",
    "#         Find loss and gradient\n",
    "        loss = torch.mean(torch.abs(y - preds))\n",
    "        hist.append(loss.detach())\n",
    "        loss.backward(inputs = (W1, b1, W2, b2))\n",
    "        \n",
    "#         Update parameters\n",
    "        W1 = W1 - lr * W1.grad\n",
    "        b1 = b1 - lr * b1.grad\n",
    "        W2 = W2 - lr * W2.grad\n",
    "        b2 = b2 - lr * b2.grad\n",
    "        \n",
    "#         New predictions\n",
    "        h = torch.max(torch.zeros((100, n_h)), x @ W1 + b1)\n",
    "        preds = h @ W2 + b2\n",
    "        \n",
    "#     Plot loss history and final predictions\n",
    "    ax2.plot(hist)\n",
    "    ax2.set_title(f\"100 iterations at lr = {lr}\")\n",
    "    plot_net(ax3, x, y, n_h, h, preds)\n",
    "\n",
    "    return W1, b1, W2, b2, hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfad8222",
   "metadata": {},
   "source": [
    "## Learning a linear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f1fee3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "x = torch.linspace(-1, 1, 100).reshape(100, 1)\n",
    "y = torch.randn(1) * x + torch.randn(1) + torch.randn(x.shape) * 0.05\n",
    "\n",
    "# Initialise parameters\n",
    "n_h, W1, b1, W2, b2 = ip(5)\n",
    "\n",
    "# Perform gradient descent\n",
    "W1, b1, W2, b2, hist = net_gd(n_h, W1, b1, W2, b2, lr=0.1, hist=[])\n",
    "W1, b1, W2, b2, hist = net_gd(n_h, W1, b1, W2, b2, lr=0.05, hist=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d01e585",
   "metadata": {},
   "source": [
    "## Learning a quadratic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68223c16",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "x = torch.linspace(-1, 1, 100).reshape(100, 1)\n",
    "y = torch.randn(1) * x**2 + torch.rand(1) + torch.randn(x.shape) * 0.05\n",
    "\n",
    "# Initialise parameters\n",
    "n_h, W1, b1, W2, b2 = ip(20)\n",
    "\n",
    "# Perform gradient descent\n",
    "W1, b1, W2, b2, hist = net_gd(n_h, W1, b1, W2, b2, lr=0.05, hist=[])\n",
    "W1, b1, W2, b2, hist = net_gd(n_h, W1, b1, W2, b2, lr=0.025, hist=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837bb697",
   "metadata": {},
   "source": [
    "## Learning a sinusoidal function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8417c63a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "x = torch.linspace(-1, 1, 100).reshape(100, 1)\n",
    "y = torch.rand(1) * torch.sin(torch.pi * x) + torch.rand(1) + torch.randn(x.shape) * 0.05\n",
    "\n",
    "# Initialise parameters\n",
    "n_h, W1, b1, W2, b2 = ip(40)\n",
    "\n",
    "# Perform gradient descent\n",
    "W1, b1, W2, b2, hist = net_gd(n_h, W1, b1, W2, b2, lr=0.01, hist=[])\n",
    "W1, b1, W2, b2, hist = net_gd(n_h, W1, b1, W2, b2, lr=0.005, hist=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc173ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
